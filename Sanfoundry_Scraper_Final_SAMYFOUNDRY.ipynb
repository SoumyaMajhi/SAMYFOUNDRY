{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!cat /etc/os-release\n",
        "#!uname -m\n",
        "!pip install pdfkit\n",
        "!wget https://github.com/wkhtmltopdf/packaging/releases/download/0.12.6-1/wkhtmltox_0.12.6-1.bionic_amd64.deb\n",
        "!cp wkhtmltox_0.12.6-1.bionic_amd64.deb /usr/bin\n",
        "!sudo apt install /usr/bin/wkhtmltox_0.12.6-1.bionic_amd64.deb"
      ],
      "metadata": {
        "id": "fRsn_rD0Phgg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install cfscrape  #installing Cloudflare-scrape\n",
        "#0. Set up requirements\n",
        "import cfscrape\n",
        "from bs4 import BeautifulSoup\n",
        "import pdfkit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VgfQkboyQmDy",
        "outputId": "9318c151-f823-4723-e04f-77fd2b0342f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting cfscrape\n",
            "  Downloading cfscrape-2.1.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: requests>=2.6.1 in /usr/local/lib/python3.9/dist-packages (from cfscrape) (2.27.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.6.1->cfscrape) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.6.1->cfscrape) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.6.1->cfscrape) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.6.1->cfscrape) (2.0.12)\n",
            "Installing collected packages: cfscrape\n",
            "Successfully installed cfscrape-2.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZVbt9DWQf1g",
        "outputId": "3eae13b4-0cf5-4153-ed19-62d6d5ccee7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Sanfoundry page URL containing all Topic links of a Chapter: https://www.sanfoundry.com/1000-data-structure-questions-answers/\n",
            "Topic  1 Total q  13\n",
            "Topic  2 Total q  10\n",
            "Topic  3 Total q  10\n",
            "Topic  4 Total q  9\n",
            "Topic  5 Total q  9\n",
            "Topic  6 Total q  9\n",
            "Topic  7 Total q  8\n",
            "Topic  8 Total q  8\n",
            "Topic  9 Total q  11\n",
            "Topic  10 Total q  10\n",
            "Topic  11 Total q  10\n",
            "Topic  12 Total q  10\n",
            "Topic  13 Total q  10\n",
            "Topic  14 Total q  10\n",
            "Topic  15 Total q  10\n",
            "Topic  16 Total q  8\n",
            "Topic  17 Total q  8\n",
            "Topic  18 Total q  10\n",
            "Topic  19 Total q  7\n",
            "Topic  20 Total q  8\n",
            "Topic  21 Total q  13\n",
            "Topic  22 Total q  10\n",
            "Topic  23 Total q  15\n",
            "Topic  24 Total q  11\n",
            "Topic  25 Total q  15\n",
            "Topic  26 Total q  10\n",
            "Topic  27 Total q  9\n",
            "Topic  28 Total q  7\n",
            "Topic  29 Total q  12\n",
            "Topic  30 Total q  13\n",
            "Topic  31 Total q  13\n",
            "Topic  32 Total q  15\n",
            "Topic  33 Total q  8\n",
            "Topic  34 Total q  11\n",
            "Topic  35 Total q  10\n",
            "Topic  36 Total q  10\n",
            "Topic  37 Total q  9\n",
            "Topic  38 Total q  14\n",
            "Topic  39 Total q  16\n",
            "Topic  40 Total q  14\n",
            "Topic  41 Total q  12\n",
            "Topic  42 Total q  10\n",
            "Topic  43 Total q  10\n",
            "Topic  44 Total q  10\n",
            "Topic  45 Total q  10\n",
            "Topic  46 Total q  10\n",
            "Topic  47 Total q  10\n",
            "Topic  48 Total q  15\n",
            "Topic  49 Total q  11\n",
            "Topic  50 Total q  11\n",
            "Topic  51 Total q  8\n",
            "Topic  52 Total q  13\n",
            "Topic  53 Total q  11\n",
            "Topic  54 Total q  12\n",
            "Topic  55 Total q  10\n",
            "Topic  56 Total q  10\n",
            "Topic  57 Total q  15\n",
            "Topic  58 Total q  10\n",
            "Topic  59 Total q  10\n",
            "Topic  60 Total q  10\n",
            "Topic  61 Total q  10\n",
            "Topic  62 Total q  14\n",
            "Topic  63 Total q  10\n",
            "Topic  64 Total q  10\n",
            "Topic  65 Total q  8\n",
            "Topic  66 Total q  15\n",
            "Topic  67 Total q  10\n",
            "Topic  68 Total q  10\n",
            "Topic  69 Total q  10\n",
            "Topic  70 Total q  10\n",
            "Topic  71 Total q  13\n",
            "Topic  72 Total q  9\n",
            "Topic  73 Total q  12\n",
            "Topic  74 Total q  12\n",
            "Topic  75 Total q  13\n",
            "Topic  76 Total q  15\n",
            "Topic  77 Total q  11\n",
            "Topic  78 Total q  15\n",
            "Topic  79 Total q  12\n",
            "Topic  80 Total q  8\n",
            "Topic  81 Total q  7\n",
            "Topic  82 Total q  8\n",
            "Topic  83 Total q  13\n",
            "Topic  84 Total q  12\n",
            "Topic  85 Total q  13\n",
            "Topic  86 Total q  10\n",
            "Topic  87 Total q  13\n",
            "Topic  88 Total q  15\n",
            "Topic  89 Total q  13\n",
            "Topic  90 Total q  10\n",
            "Topic  91 Total q  10\n",
            "Topic  92 Total q  15\n",
            "Topic  93 Total q  15\n",
            "Topic  94 Total q  10\n",
            "Topic  95 Total q  15\n",
            "Topic  96 Total q  11\n",
            "Topic  97 Total q  11\n",
            "Topic  98 Total q  12\n",
            "Topic  99 Total q  11\n",
            "Topic  100 Total q  10\n",
            "Topic  101 Total q  15\n",
            "Topic  102 Total q  10\n",
            "Topic  103 Total q  10\n",
            "Topic  104 Total q  15\n",
            "Topic  105 Total q  7\n",
            "Topic  106 Total q  15\n",
            "Topic  107 Total q  15\n",
            "Topic  108 Total q  12\n",
            "Topic  109 Total q  10\n",
            "Topic  110 Total q  10\n",
            "Topic  111 Total q  10\n",
            "Topic  112 Total q  10\n",
            "Topic  113 Total q  10\n",
            "Topic  114 Total q  10\n",
            "Topic  115 Total q  10\n",
            "End\n",
            "Total Topics Found:  115\n",
            "Total Q/A Found:  1271\n"
          ]
        }
      ],
      "source": [
        "url=input('Enter Sanfoundry page URL containing all Topic links of a Chapter: ')\n",
        "#1. Get the HTML\n",
        "scraper = cfscrape.create_scraper()  # returns a CloudflareScraper instance\n",
        "r=scraper.get(url)\n",
        "htmlContent=r.content\n",
        "\n",
        "#2. Parse the HTML\n",
        "soup=BeautifulSoup(htmlContent, 'html5lib')\n",
        "\n",
        "#3. HTML Tree Traversal\n",
        "divs=soup.find('div', class_='entry-content')\n",
        "\n",
        "lists=divs.find_all('table')\n",
        "\n",
        "filename=url.split('/')[3]\n",
        "f=open( filename + '.html','ab')  #create html file for storing the scraped data\n",
        "f.write('<!DOCTYPE html> \\n <html>\\n <head> \\n <meta charset=\"UTF-8\"> \\n </head> \\n <body>'.encode())\n",
        "f.seek(0)\n",
        "\n",
        "topics_total=0\n",
        "qna_total=0\n",
        "for l in lists:\n",
        "    topics=l.find_all('a')\n",
        "    for t in topics:\n",
        "        topics_total+=1\n",
        "        links=t['href']\n",
        "        topicnames=t.text\n",
        "        f=open( filename + '.html','ab')\n",
        "        f.write(\"\\n\\n\\t\\t\".encode()+\"Topic \".encode()+str(topics_total).encode()+\". \".encode()+topicnames.encode()+'\\n\\n'.encode())\n",
        "        f.seek(0)\n",
        "\n",
        "        url=links\n",
        "        #1. Get the HTML\n",
        "        scraper = cfscrape.create_scraper()  # returns a CloudflareScraper instance\n",
        "        r=scraper.get(url)\n",
        "        htmlContent=r.content\n",
        "\n",
        "        #2. Parse the HTML\n",
        "        soup=BeautifulSoup(htmlContent, 'html5lib')\n",
        "\n",
        "        #3. HTML Tree Traversal\n",
        "        divs=soup.find('div', class_='entry-content')\n",
        "\n",
        "        #Remove code of banner ads\n",
        "        for child in divs.find_all(\"div\", class_=\"sf-mobile-ads\"):\n",
        "            child.decompose()\n",
        "\n",
        "        for child in divs.find_all(\"div\", class_=\"sf-desktop-ads\"):\n",
        "            child.decompose()\n",
        "\n",
        "        #Remove all extra links\n",
        "        for child in divs.find_all(\"a\"):\n",
        "            child.decompose()\n",
        "\n",
        "        #Remove un necessary p tags\n",
        "        for child in divs.find_all(\"p\"):\n",
        "            # fetching text from tag and remove whitespaces\n",
        "            if len(child.text.strip()) == 0:\n",
        "                # Remove empty tag\n",
        "                child.decompose()\n",
        "\n",
        "        #Remove un necessary div tags\n",
        "        for child in divs.find_all(\"div\"):\n",
        "            # fetching text from tag and remove whitespaces\n",
        "            if len(child.text.strip()) == 0:\n",
        "                # Remove empty tag\n",
        "                child.decompose()\n",
        "\n",
        "        #removing extra divs with no class (specially sanfoundry ad links)\n",
        "        for child in divs.find_all(\"div\", class_=False):\n",
        "            child.decompose()\n",
        "\n",
        "        for child in divs.find_all(\"div\", class_=\"desktop-content\"):\n",
        "            child.decompose()\n",
        "\n",
        "        for child in divs.find_all(\"div\", class_=\"mobile-content\"):\n",
        "            child.decompose()\n",
        "\n",
        "        for child in divs.find_all(\"div\", class_=\"sf-nav-bottom\"):\n",
        "            child.decompose()\n",
        "\n",
        "        #questions\n",
        "        questions=divs.find_all('p')[1:]\n",
        "        #print(questions)\n",
        "\n",
        "        #answers\n",
        "        ans=divs.findAll('div',class_='collapseomatic_content')\n",
        "\n",
        "        #total no. of Q/A\n",
        "        questions_total=len(ans)\n",
        "        print(\"Topic \",topics_total,\"Total q \",questions_total)\n",
        "\n",
        "        for child in divs:\n",
        "            if child in ans:\n",
        "                qna_total+=1\n",
        "            if child == ans[-1]:\n",
        "                f.write(str(child).encode())\n",
        "                f.seek(0)\n",
        "                break\n",
        "            else:\n",
        "                f.write(str(child).encode())\n",
        "                f.seek(0)\n",
        "\n",
        "f.write('\\n\\n\\t\\t'.encode() + 'Total Topics Found: '.encode() + str(topics_total).encode())\n",
        "f.seek(0)\n",
        "f.write('\\n\\t\\t'.encode() + 'Total Q/A Found: '.encode() + str(qna_total).encode())\n",
        "f.seek(0)\n",
        "\n",
        "f.write('</body> \\n </html>'.encode())\n",
        "f.seek(0)\n",
        "\n",
        "f.close()\n",
        "\n",
        "#convert html to pdf\n",
        "pdfkit.from_file( filename + '.html', filename + '.pdf')\n",
        "\n",
        "print(\"End\")\n",
        "print('Total Topics Found: ',topics_total)\n",
        "print('Total Q/A Found: ',qna_total)"
      ]
    }
  ]
}